{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (V)\n",
    "This notebook trains the variational autoencoder (VAE) using data collected from 10K random rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA\n",
    "\n",
    "Load pickled data using tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../src/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = glob('{}/*'.format(DATASET_DIR))\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = pickle.load(open(filenames[0], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 96, 3), 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout_data[0][0].shape, len(rollout_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load 30% random files to train and test out autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_fnames = np.random.choice(filenames, int(len(filenames)*0.03))\n",
    "len(random_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each rollout is saved as a pickled list of tuple(state, action, next_state). Let load the data and plot a random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [row[0] for fname in random_fnames for row in pickle.load(open(fname, 'rb'))]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 96, 3), 300000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape, len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(dataset[np.random.randint(0, len(dataset))])\n",
    "plt.imshow(rollout_data[241][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use Dataset API to load and shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "\n",
    "Variational Autoencoder consists of encoder and decoder network just like vanilla autoencoder but instead of directly learning the latent vector $z$, the network learns the gaussian distribution mean and diagonal variance $(\\mu, \\sigma^2I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64,64,3)\n",
    "num_latent_vec = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Network\n",
    "\n",
    "We use conv layers for the encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_last_conv_shape: (2, 2, 256)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_image (InputLayer [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 31, 31, 32)   1568        encoder_input_image[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 14, 14, 64)   32832       conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 6, 6, 128)    131200      conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 2, 2, 256)    524544      conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1024)         0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 32)           32800       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "variance (Dense)                (None, 32)           32800       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 755,744\n",
      "Trainable params: 755,744\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = keras.Input(shape=(input_shape), name='encoder_input_image')\n",
    "x = keras.layers.Conv2D(32, 4, (2,2), activation='relu')(encoder_input)\n",
    "x = keras.layers.Conv2D(64, 4, (2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2D(128, 4, (2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2D(256, 4, (2,2), activation='relu')(x)\n",
    "encoder_last_conv_shape = K.int_shape(x)[1:]\n",
    "print(\"encoder_last_conv_shape: {}\".format(encoder_last_conv_shape))\n",
    "x = keras.layers.Flatten()(x)\n",
    "mu = keras.layers.Dense(num_latent_vec, activation='linear', name=\"mean\")(x)\n",
    "logvar = keras.layers.Dense(num_latent_vec, activation='linear', name=\"variance\")(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, [mu, logvar], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the network has learned the gaussian params, we need a way to sample the latent vector. We do this by creating a Lambda layer that accepts mean and variance as input and sampled latent vector $z$ as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(mean, logvar):\n",
    "    # reparameterizaton trick: allows gradients to pass through the sample\n",
    "    # 1. sample from unit gaussian, then\n",
    "    # 2. multiply it with standard deviation and add mean\n",
    "    e = tf.random.normal(shape=(K.shape(mean)[0], num_latent_vec))\n",
    "    return e * tf.math.exp(logvar) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_latent_vector = keras.layers.Lambda(sample)([mu, logvar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network\n",
    "For Decoder network, we sample the latent vector $z$ from the gaussian distribution and use it as input for our decoder network which is make of Deconv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DT (None, 5, 5, 128)         3276928   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 13, 13, 64)        204864    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 30, 30, 32)        73760     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 64, 64, 3)         3459      \n",
      "=================================================================\n",
      "Total params: 3,592,803\n",
      "Trainable params: 3,592,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_input = keras.layers.Input(shape=K.int_shape(sampled_latent_vector)[1:], name='decoder_input')\n",
    "x = keras.layers.Dense(np.prod(encoder_last_conv_shape))(decoder_input)\n",
    "x = keras.layers.Reshape((1,1,np.prod(encoder_last_conv_shape)))(x)\n",
    "x = keras.layers.Conv2DTranspose(128, 5, (2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2DTranspose(64, 5, (2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2DTranspose(32, 6, (2,2), activation='relu')(x)\n",
    "decoder_output = keras.layers.Conv2DTranspose(3, 6, (2,2))(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "VAEs train by maximising evidence lower bound (ELBO) on the marginal log-likelihood\n",
    "\n",
    "$log\\,p(x) >= \\mathbb{E}_{q(z|x)} [ log \\frac {p(x,z)} {q(z|x)}]$\n",
    "\n",
    "which is done by optimizing single sample Monte Carlo estimate of this expectation:\n",
    "\n",
    "$ log\\,p(x|z) + log\\,p(z) - log\\,q(z|x)$\n",
    "\n",
    "\n",
    "Original: <a href=\"https://www.tensorflow.org/beta/tutorials/generative/cvae\">source</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from tensorflow VAE example\n",
    "def log_normal_pdf(mean, var):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "def calculate_loss(mean, logvar, labels, decoded_logits):\n",
    "    xent_loss = tf.nn.softmax_cross_entropy_with_logits(labels, logits=decoded_logits)\n",
    "    logpx_z = -tf.reduce_sum(xent_loss, axis=[1,2,3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logp_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model\n",
    "\n",
    "And finally, we create a model that combines encoder and decoder to build the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            return tf.sigmoid(logits)\n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the training step that performs the following operations on every batch\n",
    "* take training data from batch and pass it through the encoder to approximate the parameters $(\\mu \\,,  \\sigma^2)$ of posterior distribution\n",
    "* sample latent variables from the posterior\n",
    "* pass the samples to the decoder network to generated decoded logits\n",
    "* calculate loss using decoded logits and training inputs\n",
    "* compute and apply gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "\n",
    "# @tf.function\n",
    "def train_step(train_x, model, optimizer):\n",
    "    # use training inputs to approximate the posterior \n",
    "    mean, logvar = model.encode(train_x)\n",
    "    # sample latent vector from the learned mean and variance\n",
    "    latent_z = sample(mean, logvar)\n",
    "    # decode z\n",
    "    decoded = model.decode(latent_z)\n",
    "    # calculate loss    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = calculate_loss(mean, logvar, labels=train_x, decoded_logits=decoded)        \n",
    "    # calculate gradients\n",
    "    gradients = tape.gradients(loss, model.trainable_variables)\n",
    "    # apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
