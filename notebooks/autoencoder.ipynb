{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (V)\n",
    "This notebook trains the variational autoencoder (VAE) using data collected from 10K random rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA\n",
    "\n",
    "Load pickled data and review a random environment state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../src/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = glob('{}/*'.format(DATASET_DIR))\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = pickle.load(open(filenames[0], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 96, 3), 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout_data[0][0].shape, len(rollout_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load 30% random files to train and test out autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAMPLE_SIZE = int(len(filenames)*0.03)\n",
    "SAMPLE_SIZE = int(len(filenames)*0.003)\n",
    "\n",
    "random_fnames = np.random.choice(filenames, SAMPLE_SIZE)\n",
    "len(random_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each rollout is saved as a pickled list of tuple(state, action, next_state). Let load the data and plot a random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [row[0] for fname in random_fnames for row in pickle.load(open(fname, 'rb'))]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 96, 3), 4000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape, len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADV1JREFUeJzt3VuMXfV1x/Hv6nicKTfZUKDGRsUgiwRFSklHFEJVRSZRE4ICiohEFEVWhTQvaUMuUgztQ9onaikK5KGKNIJGKEIJqYMKsqJEyMBDX1xMQQ1giF2obMcOEMkkKGjUsb36MNvuMD72HPvsc5v1/UijM/tyZi82/s3679ucyEwk1fIHwy5A0uAZfKkggy8VZPClggy+VJDBlwoy+FJBPQU/Ij4VEa9FxL6IuLetoiT1V5zrDTwRMQH8EvgkcBB4DvhCZr7SXnmS+mFVD++9AdiXma8DRMSPgNuB0wZ/as1UXnjFhT1sUst6e9gFnINLh13AyvHuoXeZe2culluvl+CvBw4smj4I/PnSlSJiBpgBuOCPL+BzP/hcD5vUsmaHXcA5mBl2ASvH4196vKv1ejnG7/Rb5ZTjhsyczczpzJyeWjvVw+YktaWX4B8Erlw0vQE41Fs5kgahl+A/B2yKiI0RsRq4C3iynbIk9dM5H+Nn5tGI+Bvg58AE8C+Z+XJrlUnqm15O7pGZPwV+2lItkgbEO/ekggy+VFBPQ32NiHG8dq+hsuNLBRl8qSCDLxVk8KWCPLm3AkxOTrb+M+fn51v/mRoddnypIDu+OurHKOJ05nF0MWh2fKkggy8VZPClggy+VJDBlwoy+FJBBl8qyOBLBRl8qSCDLxVk8KWCDL5UkMGXCvLpvBXg+PHjwy5BY8aOLxVk8KWCDL5UkMGXCjL4UkEGXyrI4EsFGXypIIMvFWTwpYIMvlSQwZcKWjb4EXFlRDwTEXsi4uWIuKeZf3FEPBURe5vXtf0vV1Ibuun4R4FvZOaHgBuBL0fEdcC9wM7M3ATsbKYljYFlH8vNzMPA4eb7dyNiD7AeuB34eLPaI8CzwNa+VKkzOnbs2LBL0Jg5q2P8iLgKuB7YBVze/FI48cvhsraLk9QfXQc/Ii4AfgJ8NTN/dxbvm4mI3RGxe+7I3LnUKKllXQU/IiZZCP2jmfl4M/vNiFjXLF8HvNXpvZk5m5nTmTk9tXaqjZol9aibs/oBPAzsyczvLFr0JLCl+X4L8ET75Unqh27+5t7NwJeAX0TEi828vwP+CfhxRNwN7Ac+358SJbWtm7P6/w7EaRbf0m45kgbBv7IrjYorlkwf6t+mvGVXKsiOL/VqaaceA3Z8qSCDLxXkUF81jeHwvE12fKkgO77GS/FO3RY7vlSQHX8FmJycHHYJPZlnftgllGPHlwoy+FJBBl8qyOBLBXlyT+Nl8RNrK/3S3uL/vpaf1LPjSwXZ8YdppXcsjSw7vlSQHf9c2Kk15uz4UkEGXyqo1lDfIboE2PGlkmp0fDu99D52fKmgGh1/hTt+/PiwS9CYseNLBRl8qSCDLxVk8KWCDL5UkMGXCjL4UkEGXyrI4EsFGXypoK6DHxETEfFCROxopjdGxK6I2BsRj0XE6v6VKalNZ9Px7wH2LJreBjyQmZuAI8DdbRYmqX+6Cn5EbAA+AzzUTAewGdjerPIIcEc/CpTUvm6fznsQ+CZwYTN9CfBOZh5tpg8C61uuTV06duxYKz9nYmKilZ+j0bdsx4+I24C3MvP5xbM7rJqnef9MROyOiN1zR+bOsUxJbeqm498MfDYibgWmgItYGAGsiYhVTdffwGk+5CczZ4FZgEuvu7TjLweNhrZGDhp9y3b8zLwvMzdk5lXAXcDTmflF4Bngzma1LcATfatSUqt6uY6/Ffh6ROxj4Zj/4XZKkrp0iNY/THJkXUGrfzvyrP70VmY+CzzbfP86cEN7pUgaFO/ckwqqEfxKQ0KpCzWCL+l9DL5UkMGXCjL4UkEGXyrI4EsFGXypIIMvFeSn5a4Ak5OTwy6hJ/PMD7uEcuz4UkEGXyrI4EsFGXypIIMvFWTwpYIMvlSQwZcKMvhSQQZfKsjgSwUZfKkggy8VZPClggy+VJDP468Ax48fH3YJGjN2fKkgg6/xV+kj0lr61FyDLxVU6xj/RFdo8XPGpbMyIiMTO75UUK2OL52tEenQbbPjSwUZfKkgh/paeVbo8LxNdnypoK6CHxFrImJ7RLwaEXsi4qaIuDginoqIvc3r2n4XK51RpRt5etRtx/8u8LPM/CDwEWAPcC+wMzM3ATubaUljYNngR8RFwF8CDwNk5v9m5jvA7cAjzWqPAHf0q0hJ7eqm418NvA18PyJeiIiHIuJ84PLMPAzQvF7W6c0RMRMRuyNi99yRudYKl3Tuugn+KuCjwPcy83rg95zFsD4zZzNzOjOnp9ZOnWOZktrUzeW8g8DBzNzVTG9nIfhvRsS6zDwcEeuAt/pVpM7s2LFjwy5BY2bZjp+ZvwYORMS1zaxbgFeAJ4EtzbwtwBN9qVBS67q9gedvgUcjYjXwOvDXLPzS+HFE3A3sBz7fnxIFeJlKreoq+Jn5IjDdYdEt7ZYjaRC8Zbdf7NAaYd6yKxVkx1/KTq0C7PhSQQZfKqjmUN/hvIqz40sFGXypIIMvFVTzGF8dzczOdrXe7MxMnytRv9nxpYLs+Dpp8cMYu8+wzH4//uz4UkEGXyrI4EsFGXypIE/u6aSZxZfpllzam/ES3opix5cKMvhSQQZfKshjfJ10plt2Fy/zlt3xZ8eXCjL4UkEGXyrI4EsFeXJPJ/l0Xh12fKkgO75Ouu222/5/YseO0y/T2LPjSwUZfKkgh/o66R+WDO9Pt8w798afHV8qyI6vk+zkddjxpYIMvlSQwZcK6ir4EfG1iHg5Il6KiB9GxFREbIyIXRGxNyIei4jV/S5WUjuWDX5ErAe+Akxn5oeBCeAuYBvwQGZuAo4Ad/ezUEnt6Xaovwr4w4hYBZwHHAY2A9ub5Y8Ad7RfnqR+WDb4mfkr4NvAfhYC/1vgeeCdzDzarHYQWN+vIiW1q5uh/lrgdmAjcAVwPvDpDqvmad4/ExG7I2L33JG5XmqV1JJuhvqfAN7IzLczcx54HPgYsKYZ+gNsAA51enNmzmbmdGZOT62daqVoSb3pJvj7gRsj4ryICOAW4BXgGeDOZp0twBP9KVFS27o5xt/Fwkm8/wR+0bxnFtgKfD0i9gGXAA/3sU5JLerqXv3M/BbwrSWzXwduaL0iSX3nQzoaHp8JGhpv2ZUKMvhSQZHZ8fJ7fzYWMbiNSUVlZiy3jh1fKsjgSwUZfKkgL+dVdDZnWpY9WtQ4suNLBRl8qSCDLxVk8KWCRu7kXqcbihaeBj512Yn5yy2T9H52fKmgkev4Z+rU3Sy7//77W69JWmns+FJBK+4hnRP/PR7jn4E38KxoPqQjqSODLxU0cif3Nm/eDMDTTz99yrJrr70WgAMHDgDw3nvvnbLONddc08fqVgiH7+XZ8aWCRu7k3sTEBACTk5Mn583Nzb1v3vz8/Cnv27ZtGwBbt27tvVBpjHlyT1JHI9fxx8GfdXE97HkPpDUkdnxJHdnxpRXGji+pI4MvFWTwpYIMvlTQoG/Z/Q3w++Z1nPwR41czjGfd1tybP+lmpYGe1QeIiN2ZOT3QjfZoHGuG8azbmgfDob5UkMGXChpG8GeHsM1ejWPNMJ51W/MADPwYX9LwOdSXChpY8CPiUxHxWkTsi4h7B7XdsxURV0bEMxGxJyJejoh7mvkXR8RTEbG3eV077FqXioiJiHghInY00xsjYldT82MRsXrYNS4WEWsiYntEvNrs75vGZD9/rfm38VJE/DAipkZ9Xy81kOBHxATwz8CngeuAL0TEdYPY9jk4CnwjMz8E3Ah8uan1XmBnZm4CdjbTo+YeYM+i6W3AA03NR4C7h1LV6X0X+FlmfhD4CAu1j/R+joj1wFeA6cz8MDAB3MXo7+v3y8y+fwE3AT9fNH0fcN8gtt1C7U8AnwReA9Y189YBrw27tiV1bmAhKJuBHSz8Zb3fAKs6/T8Y9hdwEfAGzXmmRfNHfT+vBw4AF7NwA9wO4K9GeV93+hrUUP/EzjrhYDNvpEXEVcD1wC7g8sw8DNC8Xja8yjp6EPgmcLyZvgR4JzOPNtOjts+vBt4Gvt8cnjwUEecz4vs5M38FfBvYDxwGfgs8z2jv61MMKvidng8e6csJEXEB8BPgq5n5u2HXcyYRcRvwVmY+v3h2h1VHaZ+vAj4KfC8zr2fhVu6RGtZ30pxzuB3YCFwBnM/CIexSo7SvTzGo4B8Erlw0vQE4NKBtn7WImGQh9I9m5uPN7DcjYl2zfB3w1rDq6+Bm4LMR8T/Aj1gY7j8IrImIE89jjNo+PwgczMxdzfR2Fn4RjPJ+BvgE8EZmvp2Z88DjwMcY7X19ikEF/zlgU3PmczULJ0OeHNC2z0osfPbWw8CezPzOokVPAlua77ewcOw/EjLzvszckJlXsbBvn87MLwLPAHc2q41azb8GDkTEtc2sW4BXGOH93NgP3BgR5zX/Vk7UPbL7uqMBnhS5Ffgl8N/A3w/75MYZ6vwLFoZp/wW82HzdysIx805gb/N68bBrPU39Hwd2NN9fDfwHsA/4V+ADw65vSa1/Cuxu9vW/AWvHYT8D/wi8CrwE/AD4wKjv66Vf3rknFeSde1JBBl8qyOBLBRl8qSCDLxVk8KWCDL5UkMGXCvo/PU4r+N2UfMoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(dataset[np.random.randint(0, len(dataset))])\n",
    "plt.imshow(rollout_data[241][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n",
    "\n",
    "Variational Autoencoder consists of encoder and decoder network just like vanilla autoencoder but instead of directly learning the latent vector $z$, the network learns the gaussian distribution mean and diagonal variance $(\\mu, \\sigma^2I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (64,64,3)\n",
    "LATENT_DIM = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Network\n",
    "\n",
    "We use conv layers for the encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_last_conv_shape: (2, 2, 256)\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_image (InputLayer [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 31, 31, 32)   1568        encoder_input_image[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 14, 64)   32832       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 6, 6, 128)    131200      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 2, 256)    524544      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 32)           32800       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "variance (Dense)                (None, 32)           32800       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 755,744\n",
      "Trainable params: 755,744\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = keras.Input(shape=(INPUT_SHAPE), name='encoder_input_image')\n",
    "x = keras.layers.Conv2D(32, 4, strides=(2,2), activation='relu')(encoder_input)\n",
    "x = keras.layers.Conv2D(64, 4, strides=(2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2D(128, 4, strides=(2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2D(256, 4, strides=(2,2), activation='relu')(x)\n",
    "encoder_last_conv_shape = K.int_shape(x)[1:]\n",
    "print(\"encoder_last_conv_shape: {}\".format(encoder_last_conv_shape))\n",
    "x = keras.layers.Flatten()(x)\n",
    "mu = keras.layers.Dense(LATENT_DIM, activation='linear', name=\"mean\")(x)\n",
    "logvar = keras.layers.Dense(LATENT_DIM, activation='linear', name=\"variance\")(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, [mu, logvar], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the network has learned the gaussian params, we need a way to sample the latent vector. We do this by creating a Lambda layer that accepts mean and variance as input and sampled latent vector $z$ as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(args):\n",
    "    mean, logvar = args\n",
    "    # reparameterizaton trick: allows gradients to pass through the sample\n",
    "    # 1. sample from unit gaussian, then\n",
    "    # 2. multiply it with standard deviation and add mean\n",
    "    e = tf.random.normal(shape=(K.shape(mean)[0], LATENT_DIM))\n",
    "    return e * tf.math.exp(logvar) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_latent_vector = keras.layers.Lambda(sample)([mu, logvar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network\n",
    "For Decoder network, we sample the latent vector $z$ from the gaussian distribution and use it as input for our decoder network which is make of Deconv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              33792     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 5, 5, 128)         3276928   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 13, 13, 64)        204864    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 30, 30, 32)        73760     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 3)         3459      \n",
      "=================================================================\n",
      "Total params: 3,592,803\n",
      "Trainable params: 3,592,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_input = keras.layers.Input(shape=K.int_shape(sampled_latent_vector)[1:], name='decoder_input')\n",
    "x = keras.layers.Dense(np.prod(encoder_last_conv_shape))(decoder_input)\n",
    "x = keras.layers.Reshape((1,1,np.prod(encoder_last_conv_shape)))(x)\n",
    "x = keras.layers.Conv2DTranspose(128, 5, strides=(2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2DTranspose(64, 5, strides=(2,2), activation='relu')(x)\n",
    "x = keras.layers.Conv2DTranspose(32, 6, strides=(2,2), activation='relu')(x)\n",
    "decoder_output = keras.layers.Conv2DTranspose(3, 6, strides=(2,2))(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "> VAEs train by maximising evidence lower bound (ELBO) on the marginal log-likelihood\n",
    "$log\\,p(x) >= \\mathbb{E}_{q(z|x)} [ log \\frac {p(x,z)} {q(z|x)}]$\n",
    "\n",
    ">which is done by optimizing single sample Monte Carlo estimate of this expectation:\n",
    "$ log\\,p(x|z) + log\\,p(z) - log\\,q(z|x)$\n",
    "\n",
    "\n",
    "Original: <a href=\"https://www.tensorflow.org/beta/tutorials/generative/cvae\">source</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from tensorflow VAE example\n",
    "def log_normal_pdf(sample, mean, var):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "def calculate_loss(mean, logvar, labels, decoded_logits):\n",
    "    xent_loss = tf.nn.softmax_cross_entropy_with_logits(labels, logits=decoded_logits)\n",
    "    z = sample([mean, logvar])\n",
    "    logpx_z = -tf.reduce_sum(xent_loss, axis=[1,2,3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logp_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Model\n",
    "\n",
    "And finally, we create a model that combines encoder and decoder to build the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        import pdb; pdb.set_trace()\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            return tf.sigmoid(logits)\n",
    "        return logits         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the training step that performs the following operations on every batch\n",
    "* take training data from batch and pass it through the encoder to approximate the parameters $(\\mu \\,,  \\sigma^2)$ of posterior distribution\n",
    "* sample latent variables from the posterior\n",
    "* pass the samples to the decoder network to generated decoded logits\n",
    "* calculate loss using decoded logits and training inputs\n",
    "* compute and apply gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step\n",
    "\n",
    "# @tf.function\n",
    "def train_step(train_x, model, optimizer):\n",
    "    # use training inputs to approximate the posterior \n",
    "    mean, logvar = model.encode(train_x)\n",
    "    # sample latent vector from the learned mean and variance\n",
    "    latent_z = sample(mean, logvar)\n",
    "    # decode z\n",
    "    decoded_logits = model.decode(latent_z)\n",
    "    # calculate loss    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = calculate_loss(mean, logvar, labels=train_x, decoded_logits=decoded_logits)        \n",
    "    # calculate gradients\n",
    "    gradients = tape.gradients(loss, model.trainable_variables)\n",
    "    # apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the numpy dataset using Dataset api from tf.data and define parameters such as batch size, buffer size to shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset generator script stored the ennvironment state as (state, action, next_state) tuple where the shape of state is (96,96,3). The encoder network expects the images to be of size (64,64,3) so we need to resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 12:24:20.609797 140170666792768 image.py:656] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the resize functionality\n",
    "imgs=tf.image.resize(dataset[:2], [64, 64])\n",
    "print(imgs.shape)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "# without normalisation\n",
    "ax[0].imshow(imgs[0])\n",
    "# Normalize to [0,1] range\n",
    "ax[1].imshow(imgs[0]/255.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images\n",
    "def preprocess_images(images):\n",
    "    images = tf.image.resize(images, [64, 64])\n",
    "    images = images/255.0\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_ds.map(preprocess_images, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the image shape\n",
    "for item in train_dataset:\n",
    "    print(item.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the ingredients in place, let's begin the training.\n",
    "The following cell shows the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible type conversion requested to type 'int32' for variable of type 'float32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1025\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: Expecting int64_t value for attr strides, got numpy.int32",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-a93a29fc90c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-19fb6d4c54de>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(train_x, model, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# use training inputs to approximate the posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# sample latent vector from the learned mean and variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlatent_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-702b99d40d71>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    868\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[1;32m   1949\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m                            name=name)\n\u001b[0m\u001b[1;32m   1952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   1032\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \"'conv2d' Op, not %r.\" % dilations)\n\u001b[1;32m   1122\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_inputs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inputs_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, default_dtype)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1369\u001b[0m       raise ValueError(\n\u001b[1;32m   1370\u001b[0m           \u001b[0;34m\"Incompatible type conversion requested to type {!r} for variable \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m           \"of type {!r}\".format(dtype.name, self.dtype.name))\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible type conversion requested to type 'int32' for variable of type 'float32'"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "EPOCHS = 10\n",
    "# Initialize the Variational Autoencoder model \n",
    "model = VAE(encoder, decoder)\n",
    "# Define optimizer\n",
    "optimizer = keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# keep track of losses\n",
    "losses = []\n",
    "\n",
    "# How often to print the loss\n",
    "print_every = int(0.1 * EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for train_x in train_ds:\n",
    "        loss = train_step(train_x, model, optimizer)\n",
    "        losses.append(loss)\n",
    "    if epoch % print_every == 0:\n",
    "        print('Epoch {}/{}: train loss {}'.format(epoch, EPOCHS, losses[-1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_images(model):\n",
    "    latent_z = tf.random.normal(shape=(4, LATENT_DIM))\n",
    "    # decode z\n",
    "    decoded = model.decode(latent_z, apply_sigmoid=True)\n",
    "    print(decoder.shape)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(decoder[i, :, :, 0])\n",
    "        ax.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.0",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
